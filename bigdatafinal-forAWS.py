# -*- coding: utf-8 -*-
"""Copy of Big_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eulGBMJpm3vkJYOnWWq7Js3InjrY4XjV

#**Customer Churn Prediction on Sparkify Dataset**
"""

#!pip install pyspark py4j

#from google.colab import drive
#drive.mount('/content/drive')

"""#### Importing Libraries"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
#import pandas as pd
#import seaborn as sns
#from matplotlib import pyplot as plt
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler

from pyspark.ml.evaluation import  MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

"""#### Creating the Spark Connection"""

sc = SparkContext.getOrCreate();
spark = SparkSession(sc)
df = spark.read.json("s3://big-data-project-final-01/sparkify_dataset.json")

"""## **Data Visualization**"""

df.count()

#pd.DataFrame(df.take(5), columns=df.columns).head()

df.printSchema()

## Number of Distinct Records
df.select('userId').distinct().count()

## Number of Distinct Authorizations
df.select('auth').distinct().count()

## Number of Distinct Levels
df.select('level').distinct().count()

## Number of Distinct gender
df.select('gender').distinct().count()

"""#### Plotting Authentication status vs Number of Users"""

# auth_df = df.select('auth').groupBy('auth').count().toPandas()
# auth_df.index = auth_df.auth
# auth_df.plot.bar()

"""#### Plotting Subscription Level (free or paid) vs Number of Users"""

# level_df = df.select('level').groupBy('level').count().toPandas()
# #level_df.plot.bar()
# level_df.index = level_df.level
# level_df.plot.bar()

"""#### Plotting Gender (Female or Male or None) vs Number of Users"""

# gender_df= df.select('gender').groupBy('gender').count().toPandas()
# gender_df.index =gender_df.gender
# #gender_df
# gender_df.plot.bar()

"""### **Data Sanity Checks**"""

df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()
#df.select([count(when(isnull(c), c)) for c in df.columns]).show()

df.where("artist is null").show()

df.where("lastName is null").show()

"""# **Data Cleaning & Prepartion**"""

## Function to Clean the Data
def datacleaning(df):
  for field in df.schema.fields:
        if field.dataType==StringType():
            df = df.withColumn(field.name,regexp_replace(field.name,'[^a-zA-Z0-9\,\-]',''))
              
  df1 = df.withColumn('interaction_time', from_unixtime(col('ts').cast(LongType())/1000).cast(TimestampType()))
  df2 = df1.withColumn('month', month(col('interaction_time')))
  #df2
  df3 = df2.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))
  df4 = df3.withColumn('userId', col('userId').cast(LongType()))
  df5 = df4.filter(col('userId').isNotNull())
  #print(df5)
  df6 = df5.filter(col('auth')!='LoggedOut')
  df7 = df6.withColumn('location', split(col('location'),',').getItem(1))
  return df7

## Function to label the data
def labelling(df):
    labelling = df.withColumn('label',when((col('page').isin(['Cancellation Confirmation','Cancel'])) | (col('auth')=='Cancelled'),1 ).otherwise(0)).groupby('userId').agg(sum('label').alias('label')).withColumn('label', when(col('label')>=1 ,1).otherwise(0))
    df = df.join(labelling, on='userId')
    #df
    return df

## Function to check the Registred days
def registered_days(df):
    lastused =  df.groupBy('userId').agg(max('ts').alias('last_interaction'))
    df = lastused.join(df, on='userId').withColumn('registered_days', ((col('last_interaction')-col('registration'))/86400000).cast(IntegerType()))
    
    return df

## Function to check the Subscription Level
def latest_level(df):
    lastlevel = df.orderBy('ts', ascending=False).groupBy('userId').agg(first('level').alias('valid_level'))
    df = df.drop('level')
    #df
    df = df.join(lastlevel, on='userId')
    return df

def avglen(df):
    averagelength = df.groupBy('userId').avg('length').withColumnRenamed('avg(length)', 'length')
    df = df.drop('length')
    #print(df)
    df = df.join(averagelength, on='userId')
    return df

## Building a Pipline
def pipeline(num_cols):
    gender =StringIndexer(inputCol='gender', outputCol='gender_index')
    location =StringIndexer(inputCol='location', outputCol='location_index')
    assembler= VectorAssembler(inputCols=num_cols, outputCol='features')
    pipeline= Pipeline(stages=[gender, location, assembler])

    return pipeline

def datapostprocess(features_df):
    num_cols = []
    
    for field in features_df.schema.fields :
        if field.dataType!=StringType():
            num_cols.append(field.name)
            #print(nums_cols)

    num_cols.remove('label')

    pipeline2 = pipeline(num_cols)
    model2 = pipeline2.fit(features_df).transform(features_df)
    #type(model_df)
    return model2

from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier
# Main Function
# Function to Predict
# We are using 3 models i.e. Logistic Regression , Random Forest and Gradient Boosting
def predictionfunction(train, test, model):
    #logistic Regression Model
    if model == 'Logistic-Regression':
        ml = LogisticRegression()
    #Random Forest Model
    elif model == 'Random-Forest':
        ml = RandomForestClassifier()
    # Gradient Boosting Model
    elif model == 'Gradient-Boost':
        ml = GBTClassifier()
    else:
        return "Model not valid"
    
    clf = ml.fit(train)
    results = clf.transform(test)
    modeleval(results)
    return clf, results

## Function to evaluate the Model
#from sklearn import metrics
def modeleval(results):
    f1_score= MulticlassClassificationEvaluator(metricName='f1')
    f1_score2= f1_score.evaluate(results.select(col('label'), col('prediction')))
    #modelsumm=metrics.classification_report(test,results.select(col('label'), col('prediction')))
    #print("The summary of the model", modelsumm)
    print('The F1 score on the test set is {:.2%}'.format(f1_score2))

def sessionduration(df):
    daily = df.groupby('userId','date','sessionId').agg(max('ts').alias('session_end'), min('ts').alias('session_start')).withColumn('session_duration_sec', (col('session_end')-col('session_start'))*0.001).groupby('userId','date').avg('session_duration_sec').groupby('userId').agg(mean('avg(session_duration_sec)').alias('avg_daily_session_duration')).orderBy('userId', ascending=False)

    monthly = df.groupby('userId','month','sessionId').agg(max('ts').alias('session_end'), min('ts').alias('session_start')).withColumn('session_duration_sec', (col('session_end')-col('session_start'))*0.001).groupby('userId','month').avg('session_duration_sec').groupby('userId').agg(mean('avg(session_duration_sec)').alias('avg_monthly_session_duration')).orderBy('userId', ascending=False)
    
    return daily.join(monthly, on='userId')

def item_aggregates(df):
    daily2 = df.groupby('userId','date').agg(max('itemInSession')).groupBy('userId').avg('max(itemInSession)').withColumnRenamed('avg(max(itemInSession))', 'avg_daily_items')
    #daily2
    monthly3 = df.groupby('userId','month').agg(max('itemInSession')).groupBy('userId').avg('max(itemInSession)').withColumnRenamed('avg(max(itemInSession))', 'avg_monthly_items')
    
    return daily2.join(monthly3, on='userId')

def pageevent_dailyormonthly(df):
    
    # Function to calculate the daily averages for each user except the ones that include cancel
    pg_daily_df,exp_dict = daily_pg(df)
    # Function to calculate the monthly averages for each user except the ones that include cancel
    pg_monthly_df = monthly_pg(df,exp_dict)

    return pg_daily_df.join(pg_monthly_df, on='userId')

def daily_pg(df):
    listOfDistinctPages =[row.page for row in df.select('page').distinct().collect()]
    listOfDistinctPages.remove('Cancel')
    listOfDistinctPages.remove('CancellationConfirmation')
    daily_page_event_df = df.groupby('userId','date').pivot('page').count()
    exp_dict={}
    for page in listOfDistinctPages:
        exp_dict.update({page:'mean'})
    daily_page_event_df = daily_page_event_df.join(daily_page_event_df.groupBy('userId').agg(exp_dict).fillna(0), on='userId')

    for page in listOfDistinctPages:
        daily_page_event_df =daily_page_event_df.drop(page)  
        daily_page_event_df= daily_page_event_df.withColumnRenamed('avg({})'.format(page), 'avg_daily_{}'.format(page))

    pg_daily_df = daily_page_event_df.drop('Cancel','CancellationConfirmation','date').drop_duplicates()
    return pg_daily_df,exp_dict

def monthly_pg(df,exp_dict):
    listOfDistinctPages =[row.page for row in df.select('page').distinct().collect()]
    listOfDistinctPages.remove('Cancel')
    listOfDistinctPages.remove('CancellationConfirmation')
    # exp_dict={}
    monthly_page_event_df = df.groupby('userId','month').pivot('page').count()

    monthly_page_event_df = monthly_page_event_df.join(monthly_page_event_df.groupBy('userId').agg(exp_dict).fillna(0), on='userId')
    for page in listOfDistinctPages:
        monthly_page_event_df = monthly_page_event_df.drop(page)    
        monthly_page_event_df = monthly_page_event_df.withColumnRenamed('avg({})'.format(page), 'avg_monthly_{}'.format(page))

    pg_monthly_df = monthly_page_event_df.drop('Cancel','CancellationConfirmation','month').drop_duplicates()
    return pg_monthly_df

# def page1(df):
#   pages_list = [r.page for r in df.select("page")]
#   pages_list.remove('CancellationConfirmation')
#   pages_list.remove('Cancellation')

def mergefeature(df, df_sess_duration, df_item, df_page):

    alljoined =df_sess_duration.join(df_item, on='userId').join(df_page, on='userId')
    #drop the following features
    df = df.drop('auth', 'level','length','userAgent','month','date','interaction_time','registration', 'ts','song','page','itemInSession','sessionId','artist','firstName','lastName','method','status')
    finaljoined = alljoined.join(df, on='userId')
    
    finaljoined2 = finaljoined.drop_duplicates()
    features = finaljoined2.drop('userId')
    
    return features

vali = datacleaning(df)
labell= labelling(vali)
regd= registered_days(labell)
df = latest_level(regd)

df.toJSON("Clean_Sparkify_Data.json")

# df_count = df.select('userId','gender','valid_level').groupby('gender','valid_level').count().toPandas().sort_values(by='count')
# ## Plotting Free or Paid Subscription vs Male or Female users
# fig = plt.figure(figsize=(8,8))
# ax = fig.gca()
# plt.rcParams["font.size"] = "20"
# df_count.pivot(index='valid_level', columns='gender', values='count').plot(kind='bar', ax=ax)

# df_count_ = df.select('userId','valid_level','label').groupby('valid_level','label').count().toPandas().sort_values(by='count')
# fig = plt.figure(figsize=(8,8))
# ax = fig.gca()
# plt.rcParams["font.size"] = "20"
# df_count_.pivot(index='label', columns='valid_level', values='count').plot(kind='bar', ax=ax)

df = avglen(df)
session =sessionduration(df)
aggri =item_aggregates(df)
#df_item
page= pageevent_dailyormonthly(df)
#print(df_page)

df = mergefeature(df,session,aggri,page)

df.show()

"""# **Data Modeling**"""

finalmodeldata = datapostprocess(df)

## Spliting data into 70-30 Ratio (i.e. 70% Train Dataset, 30% Test Dataset with sandom seed=69)
X_train, X_test = finalmodeldata.randomSplit([0.7, 0.3], seed=69)

# for model in ['logistic_regression','random_forest','gradient_boosting']:
#     predictionfunction(train, test, model)

model1='Logistic-Regression'
predictionfunction(X_train,X_test,model1)

model2='Random-Forest'
predictionfunction(X_train,X_test,model2)

model3='Gradient-Boost'
predictionfunction(X_train,X_test,model3)

